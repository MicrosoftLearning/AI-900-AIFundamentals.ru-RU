{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Обнаружение объектов\r\n",
        "\r\n",
        "*Обнаружение объектов* — это форма компьютерного зрения, в которой модель машинного обучения обучена классифицировать отдельные экземпляры объектов на изображении и обозначает *ограничивающие прямоугольники*, отмечающие местоположение объектов. Можно рассматривать это как переход от *классификации изображений* (при которой модель отвечает на вопрос «Что это за изображение?») к построению решений, где мы можем спросить модель «Какие объекты на этом изображении, и где они находятся?».\r\n",
        "\r\n",
        "![Робот, идентифицирующий фрукт](./images/object-detection.jpg)\r\n",
        "\r\n",
        "Например, в продуктовом магазине может быть использована модель обнаружения объектов для реализации автоматизированной системы контроля, которая сканирует конвейерную ленту с помощью камеры и может идентифицировать конкретные предметы без необходимости размещения каждого предмета на ленте и сканирования их по отдельности.\r\n",
        "\r\n",
        "Когнитивная служба **Пользовательское визуальное распознавание** в Microsoft Azure предоставляет облачное решение для создания и публикации пользовательских моделей обнаружения объектов.\r\n",
        "\r\n",
        "## Создание ресурса пользовательского визуального распознавания\r\n",
        "\r\n",
        "Чтобы использовать службу пользовательского визуального распознавания, вам нужен ресурс Azure, который вы можете использовать для обучения модели, и ресурс, с помощью которого вы можете опубликовать ее для использования приложениями. Можно использовать один и тот же ресурс для каждой из этих задач или различные ресурсы для каждой из них, чтобы распределить затраты по отдельности, при условии, что оба ресурса созданы в одном и том же регионе. Ресурс для одной (или обеих) задач может быть общим ресурсом **когнитивных служб Cognitive Services** или специфическим ресурсом **службы пользовательского визуального распознавания**. Используйте следующие инструкции для создания нового ресурса **пользовательского визуального распознавания** (или вы можете использовать существующий ресурс, если он у вас есть).\r\n",
        "\r\n",
        "1. В новой вкладке браузера войдите на портал Azure по адресу: [https://portal.azure.com](https://portal.azure.com), используя учетную запись Майкрософт, связанную с вашей подпиской Azure.\r\n",
        "2. Нажмите кнопку **&#65291;Создать ресурс**, выполните поиск по запросу *пользовательское визуальное распознавание* и создайте ресурс **Пользовательское визуальное распознавание** со следующими параметрами:\r\n",
        "    - **Параметры создания**: Обе\r\n",
        "    - **Подписка**. *Ваша подписка Azure*\r\n",
        "    - **Группа ресурсов**. *Выберите или создайте группу ресурсов с уникальным именем.*\r\n",
        "    - **Имя**. *Введите уникальное имя*\r\n",
        "    - **Место проведения обучения**: *Выберите любой доступный регион*\r\n",
        "    - **Ценовая категория обучения**: F0\r\n",
        "    - **Место прогнозирования**. *То же, что и у ресурса обучения*\r\n",
        "    - **Ценовая категория для прогнозирования**. F0\r\n",
        "\r\n",
        "    > **Примечание**: Если в вашей подписке уже есть служба пользовательского визуального распознавания F0, выберите **S0** для этого.\r\n",
        "\r\n",
        "3. Дождитесь, пока завершится создание ресурса.\r\n",
        "\r\n",
        "## Создайте проект службы «Пользовательское визуальное распознавание».\r\n",
        "\r\n",
        "Для обучения модели обнаружения объектов вам необходимо создать проект службы «Пользовательское визуальное распознавание» на основе вашего ресурса обучения. Для этого вы будете использовать портал «Пользовательское визуальное распознавание».\r\n",
        "\r\n",
        "1. В новой вкладке браузера войдите на портал «Пользовательское визуальное распознавание» по адресу: [https://customvision.ai](https://customvision.ai), используя учетную запись Майкрософт, связанную с вашей подпиской Azure.\r\n",
        "2. Создайте новый проект со следующими параметрами.\r\n",
        "    - **Имя**. Обнаружение продуктов\r\n",
        "    - **Описание**. Обнаружение объектов для продуктов.\r\n",
        "    - **Ресурс**: *Ресурс пользовательского визуального распознавания, который вы создали ранее*\r\n",
        "    - **Типы проектов**. Обнаружение объектов\r\n",
        "    - **Домены**: Общие\r\n",
        "3. Подождите, пока проект будет создан и открыт в браузере.\r\n",
        "\r\n",
        "## Как добавить и пометить изображения\r\n",
        "\r\n",
        "Для обучения модели обнаружения объектов нужно загрузить изображения, содержащие классы, которые вы хотите, чтобы модель идентифицировала, и пометить их, чтобы обозначить ограничивающие прямоугольники для каждого экземпляра объекта.\r\n",
        "\r\n",
        "1. Скачайте и извлеките обучающие изображения по адресу: https://aka.ms/fruit-objects. Извлеченная папка содержит коллекцию изображений фруктов. **Примечание.** Если доступ к обучающим изображениям невозможен, в качестве временного решения перейдите на страницу https://www.github.com и затем https://aka.ms/fruit-objects. \r\n",
        "2. Убедитесь на портале «Пользовательское визуальное распознавание» [https://customvision.ai](https://customvision.ai), что вы работаете в своем проекте обнаружения объектов _Grocery Detection_. Затем выберите **Добавить изображения** и выгрузите все изображения в извлеченной папке.\r\n",
        "\r\n",
        "![Выгрузите загруженные изображения, нажимая «Добавить изображения».](./images/fruit-upload.jpg)\r\n",
        "\r\n",
        "3. После того, как изображения будут загружены, выберите первое, чтобы открыть его.\r\n",
        "4. Удерживайте курсор мыши над любым объектом на изображении до тех пор, пока не отобразится автоматически обнаруженная область, как показано на изображении ниже. Затем выберите объект и при необходимости измените размер области, чтобы окружить его.\r\n",
        "\r\n",
        "![Область по умолчанию для объекта](./images/object-region.jpg)\r\n",
        "\r\n",
        "Или же можно просто обвести объект, чтобы создать область.\r\n",
        "\r\n",
        "5. После того как область окружит объект, добавьте новый тег с соответствующим типом объекта (*яблоко*, *банан* или *апельсин*), как показано здесь:\r\n",
        "\r\n",
        "![Помеченный объект на изображении](./images/object-tag.jpg)\r\n",
        "\r\n",
        "6. Выберите и пометьте каждый объект на изображении, изменив размер областей и добавив новые теги по мере необходимости.\r\n",
        "\r\n",
        "![Два помеченных объекта на изображении](./images/object-tags.jpg)\r\n",
        "\r\n",
        "7. Используйте ссылку **>** справа, чтобы перейти к следующему изображению и пометить его объекты. Затем просто продолжайте работать над всей коллекцией изображений, пометив каждое яблоко, банан и апельсин.\r\n",
        "\r\n",
        "8. По окончании тегирования последнего изображения закройте редактор **Подробности изображения** и на странице **Обучающие изображения** в разделе **Теги** выберите пункт **Помеченные**, чтобы просмотреть все помеченные изображения:\r\n",
        "\r\n",
        "![Помеченные изображения в проекте](./images/tagged-images.jpg)\r\n",
        "\r\n",
        "## Обучение и тестирование модели\r\n",
        "\r\n",
        "Теперь, когда вы пометили изображения в своем проекте, вы готовы обучить модель.\r\n",
        "\r\n",
        "1. В проекте пользовательского визуального распознавания нажмите кнопку **Обучение** над изображениями, чтобы обучить модель обнаружения с помощью помеченных изображений. Выберите параметр **Быстрое обучение**.\r\n",
        "2. Дождитесь окончания обучения (это может занять около десяти минут), а затем просмотрите показатели производительности *Точность*, *Полнота* и *mAP* — они измеряют точность прогнозирования классификационной модели, и все они должны быть высокими.\r\n",
        "3. В правом верхнем углу страницы нажмите кнопку **Быстрый тест**, а затем в окне **URL-адрес изображения** введите `https://aka.ms/apple-orange` и просмотрите прогноз, который генерируется. Затем закройте окно **Быстрый тест**.\r\n",
        "\r\n",
        "## Публикация и использование модели обнаружения объектов\r\n",
        "\r\n",
        "Теперь вы готовы опубликовать свою обученную модель и использовать ее из клиентского приложения.\r\n",
        "\r\n",
        "1. В верхней левой части страницы **Производительность** нажмите **&#128504; Опубликовать**, чтобы опубликовать обученную модель со следующими параметрами:\r\n",
        "    - **Название модели**: detect-produce\r\n",
        "    - **Ресурс прогнозирования**: *Ваш ресурс **прогнозирования** пользовательского визуального распознавания*.\r\n",
        "\r\n",
        "### (!) Проверка \r\n",
        "Вы использовали то же название модели: **detect-produce**? \r\n",
        "\r\n",
        "2. После публикации нажмите на значок *Настройки* (&#9881;) в правом верхнем углу страницы **Производительность**, чтобы просмотреть настройки проекта. Затем в разделе **Общие** (слева) скопируйте **идентификатор проекта**. Прокрутите вниз и вставьте его в ячейку с кодом, указанную ниже в шаге 5, заменив **YOUR_PROJECT_ID**. \r\n",
        "\r\n",
        "> (*если в начале этого упражнения вы использовали ресурс **Cognitive Services** вместо создания ресурса **пользовательского визуального распознавания**, можно скопировать его ключ и конечную точку из правой части настроек проекта, вставить их в ячейку с кодом ниже и выполнить его, чтобы увидеть результаты. В противном случае, продолжайте выполнение описанных ниже шагов, чтобы получить ключ и конечную точку для вашего ресурса прогнозирования пользовательского визуального распознавания*).\r\n",
        "\r\n",
        "3. В левом верхнем углу страницы **Настройки проекта** нажмите на значок *Галерея проектов* (&#128065;), чтобы вернуться на главную страницу портала «Пользовательское визуальное распознавание», где теперь находится ваш проект.\r\n",
        "\r\n",
        "4. На главной странице портала «Пользовательское визуальное распознавание», в правом верхнем углу, нажмите на значок *Настройки* (&#9881;), чтобы просмотреть настройки своей службы пользовательского визуального распознавания. Затем в разделе **Ресурсы** разверните свой ресурс *прогнозирования* (<u>не</u> ресурс обучения) и скопируйте его значения **Ключ** и **Конечная точка** в ячейку с кодом, указанную ниже в шаге 5, заменив **YOUR_KEY** и **YOUR_ENDPOINT**.\r\n",
        "\r\n",
        "### (!) Проверка \r\n",
        "Если вы используете ресурс **пользовательского визуального распознавания**, использовали ли вы ресурс **прогнозирования** (<u>не</u> ресурс обучения)?\r\n",
        "\r\n",
        "5. Выполните код в расположенной ниже ячейке с кодом, нажав на кнопку «Выполнить код в ячейке» <span>&#9655;</span> (слева от ячейки), чтобы установить переменные в соответствие с идентификатором проекта, ключом и значениями конечной точки."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "project_id = 'YOUR_PROJECT_ID' # Replace with your project ID\r\n",
        "cv_key = 'YOUR_KEY' # Replace with your prediction resource primary key\r\n",
        "cv_endpoint = 'YOUR_ENDPOINT' # Replace with your prediction resource endpoint\r\n",
        "\r\n",
        "model_name = 'detect-produce' # this must match the model name you set when publishing your model iteration exactly (including case)!\r\n",
        "print('Ready to predict using model {} in project {}'.format(model_name, project_id))"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1599692485387
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь вы можете использовать ключ и конечную точку с клиентом службы пользовательского визуального распознавания для подключения к вашей модели обнаружения объектов пользовательского визуального распознавания.\r\n",
        "\r\n",
        "Выполните код в следующей ячейке, которая использует вашу модель для обнаружения отдельных объектов на изображении.\r\n",
        "\r\n",
        "> **Примечание**. Не стоит волноваться по поводу содержимого кода. Для службы пользовательского визуального распознавания в нем используется Python SDK, чтобы отправить изображение вашей модели и получить прогнозы для обнаруженных объектов. Каждый прогноз состоит из имени класса (*яблоко*, *банан* или *апельсин*) и координат *ограничивающего прямоугольника*, указывающего, где на изображении был обнаружен спрогнозированный объект. Затем код использует эту информацию, чтобы нарисовать помеченное окошко вокруг каждого объекта на изображении."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "from azure.cognitiveservices.vision.customvision.prediction import CustomVisionPredictionClient\r\n",
        "from msrest.authentication import ApiKeyCredentials\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "from PIL import Image, ImageDraw, ImageFont\r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "%matplotlib inline\r\n",
        "\r\n",
        "# Load a test image and get its dimensions\r\n",
        "test_img_file = os.path.join('data', 'object-detection', 'produce.jpg')\r\n",
        "test_img = Image.open(test_img_file)\r\n",
        "test_img_h, test_img_w, test_img_ch = np.array(test_img).shape\r\n",
        "\r\n",
        "# Get a prediction client for the object detection model\r\n",
        "credentials = ApiKeyCredentials(in_headers={\"Prediction-key\": cv_key})\r\n",
        "predictor = CustomVisionPredictionClient(endpoint=cv_endpoint, credentials=credentials)\r\n",
        "\r\n",
        "print('Detecting objects in {} using model {} in project {}...'.format(test_img_file, model_name, project_id))\r\n",
        "\r\n",
        "# Detect objects in the test image\r\n",
        "with open(test_img_file, mode=\"rb\") as test_data:\r\n",
        "    results = predictor.detect_image(project_id, model_name, test_data)\r\n",
        "\r\n",
        "# Create a figure to display the results\r\n",
        "fig = plt.figure(figsize=(8, 8))\r\n",
        "plt.axis('off')\r\n",
        "\r\n",
        "# Display the image with boxes around each detected object\r\n",
        "draw = ImageDraw.Draw(test_img)\r\n",
        "lineWidth = int(np.array(test_img).shape[1]/100)\r\n",
        "object_colors = {\r\n",
        "    \"apple\": \"lightgreen\",\r\n",
        "    \"banana\": \"yellow\",\r\n",
        "    \"orange\": \"orange\"\r\n",
        "}\r\n",
        "for prediction in results.predictions:\r\n",
        "    color = 'white' # default for 'other' object tags\r\n",
        "    if (prediction.probability*100) > 50:\r\n",
        "        if prediction.tag_name in object_colors:\r\n",
        "            color = object_colors[prediction.tag_name]\r\n",
        "        left = prediction.bounding_box.left * test_img_w \r\n",
        "        top = prediction.bounding_box.top * test_img_h \r\n",
        "        height = prediction.bounding_box.height * test_img_h\r\n",
        "        width =  prediction.bounding_box.width * test_img_w\r\n",
        "        points = ((left,top), (left+width,top), (left+width,top+height), (left,top+height),(left,top))\r\n",
        "        draw.line(points, fill=color, width=lineWidth)\r\n",
        "        plt.annotate(prediction.tag_name + \": {0:.2f}%\".format(prediction.probability * 100),(left,top), backgroundcolor=color)\r\n",
        "plt.imshow(test_img)\r\n"
      ],
      "outputs": [],
      "metadata": {
        "gather": {
          "logged": 1599692585672
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Просмотрите полученные прогнозы, которые показывают обнаруженные объекты и вероятность для каждого прогноза."
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}